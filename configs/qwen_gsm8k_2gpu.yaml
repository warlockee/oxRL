########################################################################
# Example: Post-train Qwen2.5-0.5B-Instruct on GSM8K with SGRPO
#
# To onboard a new task, you need:
#   1. A preprocessing script  → see preprocessing/gsm8k.py
#   2. A reward function       → see rewards/compute_score.py::gsm8k_reward_func
#   3. This config file        → copy, change model/data/reward_func
#
# Preprocess:
#   python preprocessing/gsm8k.py \
#     --local_dir /ceph/workspace/oxrl/data \
#     --run_id qwen05b --use_system_prompt True
#
# Train (uses 2 GPUs total: 1 training + 1 rollout):
#   python main_rl.py --config-file configs/qwen_gsm8k_2gpu.yaml
########################################################################

run:
  experiment_id: "qwen05b_gsm8k"
  project_name: "oxrl-exp"
  tracking_uri: "http://localhost:5000"
  # 1 GPU for training (deepspeed) + 1 GPU for rollout (vllm) = 2 total
  training_gpus: 1
  rollout_gpus: 1
  ray_address: null
  ray_master_port: 29500
  distributed_training_strategy: "deepspeed-zero3"
  checkpoint_dir: "/ceph/workspace/oxrl/ckps"
  seed: 42

train:
  alg_name: "sgrpo"
  optimizer_name: "adamw"
  # Lower lr for small model fine-tuning
  lr: 1e-6
  adam_epsilon: 1e-8
  betas: [0.9, 0.95]
  weight_decay: 0.01
  warmup_steps_ratio: 0.1
  clip_grad_norm: 1.0
  lr_scheduler: "WarmupCosineLR"

  kl_coeff: 0.0
  clip_low: -0.2
  clip_high: 0.2
  entropy_coeff: 0.0
  update_after_full_replay: True

  total_number_of_epochs: 30
  train_steps_per_epoch: 5
  dynamic_ratio_every_step: True

  train_batch_size_per_gpu: 2
  gradient_accumulation_steps: 1
  val_batch_size_per_gpu: 16
  normalize_loss: True

reward:
  broadcast: False
  eps_reward_norm: 1e-8
  # Points to gsm8k_reward_func in rewards/compute_score.py
  reward_func: gsm8k_reward_func

rollout:
  temperature: 1.0
  # GSM8K chain-of-thought needs room
  max_tokens: 1024
  # Number of responses sampled per prompt for advantage estimation
  n_samples: 8
  top_p: 1.0
  top_k: -1
  ignore_eos: False
  stop: ""
  gpu_memory_utilization: 0.5
  stop_token_ids: []
  prompt_logprobs: False
  force_strict_on_policy: True
  tensor_parallel_size: 1
  rollout_batch_size_per_gpu: 2

model:
  name: "Qwen/Qwen2.5-0.5B-Instruct"
  dtype: "bfloat16"
  ref_model: ""
  ref_model_offload_to_cpu: True
  trust_remote_code: True
  use_cache: False
  model_class: "llm"
  gradient_checkpointing: True
  attn_implementation: "flash_attention_2"

data:
  train_dnames: ["gsm8k_qwen05b_wsp_train"]
  train_ratios: {"gsm8k_qwen05b_wsp_train": 1.0}
  train_files_path: "/ceph/workspace/oxrl/data/gsm8k_qwen05b_wsp_train.parquet"
  val_files_path: "/ceph/workspace/oxrl/data/gsm8k_qwen05b_wsp_test.parquet"
  num_workers: 4
  # Must be >= max_tokens so prompt + response fits
  max_seq_len: 1024
  prompt_key: "prompt"
  # Column in parquet that contains the ground-truth answer.
  # Passed to reward function as metadata["answer"].
  answer_key: "answer"

deepspeed:
  zero_optimization:
    stage: 3
    stage3_param_persistence_threshold: 100000.0
    stage3_prefetch_bucket_size: 50000000.0

    offload_optimizer:
      device: "none"
      pin_memory: true
    offload_param:
      device: "none"
      pin_memory: true

    contiguous_gradients: true
    overlap_comm: true
    reduce_scatter: true
    reduce_bucket_size: 500000000.0
    allgather_bucket_size: 500000000.0
    stage3_gather_16bit_weights_on_model_save: true

  activation_checkpointing:
    partition_activations: true
    contiguous_memory_optimization: true

  steps_per_print: 100
  wall_clock_breakdown: false

  flops_profiler:
    enabled: false
    profile_step: 10
    module_depth: -1
    top_modules: 1
    detailed: true
    output_file: null

inference_engine:
  name: "vllm"
