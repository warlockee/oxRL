run:
  experiment_id: "dummyrun_rl_1"
  project_name: "oxrl-exp"
  tracking_uri: "http://localhost:5000"
  distributed_training_strategy: "deepspeed-zero3"
  checkpoint_dir: "/ceph/workspace/oxrl/ckps"
  seed: 42

train:
  ###############
  # optimizer related arguments
  ###############
  alg_name: "sft"

  # The optimizer related arguments will be automatically copied to deepspeed config or others
  # distributed training frameworks. this way make it easier to handle them in one place and avoid any duplication.
  optimizer_name: "adamw"
  lr: 1e-5
  adam_epsilon: 1e-8
  betas: [0.9, 0.95]
  weight_decay: 0.01
  warmup_steps_ratio: 0.1
  clip_grad_norm: 1.0
  lr_scheduler: "WarmupCosineLR"

  ###############
  # general training loop arguments
  ###############
  # Here, an "epoch" is defined by a fixed number of micro-batch iterations, not a full sweep of the dataset.
  # Each epoch processes: micro_batches_per_epoch * batch_size_per_gpu * world_size samples.
  total_number_of_epochs: 30
  # number of micro-batch iterations per epoch (optimizer_steps = micro_batches_per_epoch // gradient_accumulation_steps)
  micro_batches_per_epoch: 1000
  # if True, we will recalculate the ratio of each dataset in every step
  dynamic_ratio_every_step: True

  ###############
  # Arguments which are common to both deepspeed or other distributed training frameworks and standalone training.
  # We keep them here in case with add other distributed training frameworks in the future.
  ###############
  # Note: train_batch_size_per_gpu is same as train_micro_batch_size_per_gpu in deepspeed.
  # effective / global batch_size would be train_batch_size_per_gpu * gradient_accumulation_steps * number_of_gpus.
  # exp: micro * accum * world_size. 2 * 16 * 1 = 32 effective batch size.
  train_batch_size_per_gpu: 2
  gradient_accumulation_steps: 1
  val_batch_size_per_gpu: 16

  # False: Uses sum of losses.
  normalize_loss: True

model:
  name: "google/gemma-3-4b-it"
  dtype: "bfloat16"
  ref_model: ""
  ref_model_offload_to_cpu: True
  trust_remote_code: False
  use_cache: False
  model_class: "llm"
  gradient_checkpointing: True
  # flash_attention_2 or eager
  attn_implementation: "flash_attention_2"

data:
  train_dnames: ["dataset1", "dataset2"]
  train_ratios: {"dataset1": 0.8, "dataset2": 0.2}
  train_files_path: "/ceph/workspace/oxrl/data/gsm8k_123245_ns_train.parquet"
  val_files_path: "/ceph/workspace/oxrl/data/gsm8k_123245_ns_test.parquet"
  num_workers: 4
  max_seq_len: 512
  prompt_key: "prompt"
  answer_key: "answer"
 
deepspeed:
  zero_optimization:
    # 0:Disabled, 1:OptState, 2:Opt+Grad, 3:Opt+Grad+Param (Max VRAM savings)
    stage: 3

    # Threshold for keeping small parameters on GPU. Default 1e8.
    # Prevents "thrashing" (constant PCIe transfer) for small layers (like LayerNorm).
    stage3_param_persistence_threshold: 100000.0

    # Controls how much data is pre-fetched from CPU to GPU.
    # Larger = faster but more VRAM.
    stage3_prefetch_bucket_size: 50000000.0

    # --- CPU Offloading ---
    # Offload optimizer to CPU. Use "none" if GPU VRAM suffices.
    offload_optimizer:
      device: "none"
      pin_memory: true

    # Offload params to CPU. High performance penalty; use "none" if possible.
    offload_param:
      device: "none"
      pin_memory: true

    # --- Communication ---
    # Reduces fragmentation by copying gradients to contiguous buffer
    contiguous_gradients: true

    # Attempts to overlap the reduction (communication) of gradients
    # with the backward pass computation. Boosts speed.
    overlap_comm: true

    # In ZeRO, "reduce scatter" is how gradients are averaged and scattered
    # to the specific GPU responsible for updating that specific weight.
    reduce_scatter: true

    # --- Tuning Knobs ---
    # Controls the size of the chunks of data sent between GPUs.
    # Larger = higher bandwidth efficiency but higher peak memory usage.
    # 5e8 (500MB) is a standard aggressive default.
    reduce_bucket_size: 500000000.0
    allgather_bucket_size: 500000000.0

    # --- Saving / Checkpointing ---
    # If true, gathers the fragmented ZeRO-3 weights into a single FP16 file when saving
    stage3_gather_16bit_weights_on_model_save: true

  # --- Activation Checkpointing ---
  # Trade compute for VRAM by recomputing activations for the backward pass
  activation_checkpointing:
    # Only stores the activation for the specific partition of the model handled by this GPU
    partition_activations: true

    # Ensures activation memory is contiguous, reducing fragmentation.
    contiguous_memory_optimization: true

  # --- Logging ---
  steps_per_print: 100
  # If true, prints a detailed timing breakdown of fwd/bwd/step time.
  # Useful for debugging speed, but noisy.
  wall_clock_breakdown: false

  # Measure Model Flops Utilization
  flops_profiler:
    enabled: false
    profile_step: 10
    module_depth: -1
    top_modules: 1
    detailed: true
    output_file: null

inference_engine:
  name: "vllm"