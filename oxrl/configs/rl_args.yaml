run:
  experiment_id: "dummyrun_rl_1"
  project_name: "oxrl-exp"
  tracking_uri: ""
  training_gpus: 2
  rollout_gpus: 2
  ray_address: null #"auto" if it is the same node, otherwise run ray start --head --port=6379 and assign ray_address to "auto"
  # Port for torch distributed/deepspeed rendezvous.
  ray_master_port: 29500
  distributed_training_strategy: "deepspeed-zero3"
  checkpoint_dir: "./checkpoints"
  seed: 42

train:
  ###############
  # optimizer related arguments
  ###############
  # Algorithm options:
  #   sgrpo  - Token-level clipped surrogate (default). Standard dense models.
  #   gspo   - Sequence-level clipped surrogate. Use for MoE models to absorb
  #            routing noise between vLLM and HF/DeepSpeed.
  #   cispo  - Clipped ratio as weight on log-prob. More conservative updates,
  #            use when sgrpo shows reward hacking or training instability.
  #   ppo    - Full PPO with value head (GAE, critic loss). Higher memory/compute
  #            cost. Use when you need fine-grained credit assignment.
  #   rlhf   - Same as sgrpo. Alias for readability with reward-model setups.
  #   rlaif  - Same as sgrpo. Alias for readability with AI-feedback setups.
  alg_name: "sgrpo"

  # The optimizer related arguments will be automatically copied to deepspeed config or others
  # distributed training frameworks. this way make it easier to handle them in one place and avoid any duplication.
  optimizer_name: "adamw"
  lr: 1e-5
  adam_epsilon: 1e-8
  betas: [0.9, 0.95]
  weight_decay: 0.01
  warmup_steps_ratio: 0.1
  clip_grad_norm: 1.0
  lr_scheduler: "WarmupCosineLR"

  # Specific policy arguments
  kl_coeff: 0.0
  clip_low: 0.2
  clip_high: 0.2
  entropy_coeff: 0.0
  update_after_full_replay: True

  ###############
  # general training loop arguments
  ###############
  # Here, an "epoch" is defined by a fixed number of training steps, not a full sweep of the dataset.
  # Each epoch processes: train_steps_per_epoch * global_batch_size samples.
  total_number_of_epochs: 30
  # Number of training steps per epoch. Set to 1 for strict on-policy training
  # (resample before every gradient update). Higher values reuse rollout data.
  train_steps_per_epoch: 5

  ###############
  # Arguments which are common to both deepspeed or other distributed training frameworks and standalone training.
  # We keep them here in case with add other distributed training frameworks in the future.
  ###############
  # Note: train_batch_size_per_gpu is same as train_micro_batch_size_per_gpu in deepspeed.
  # effective / global batch_size would be train_batch_size_per_gpu * gradient_accumulation_steps * number_of_gpus.
  # exp: micro * accum * world_size. 2 * 16 * 1 = 32 effective batch size.
  # batch size for sampling from the replay buffer during training.
  train_batch_size_per_gpu: 2
  gradient_accumulation_steps: 1
  val_batch_size_per_gpu: 16

  # False: Uses sum of losses.`
  normalize_loss: True

reward:
  broadcast: False
  eps_reward_norm: 1e-8
  # Reward function options:
  #   default_reward_func    - 1.0 if model stopped naturally (EOS), 0.0 otherwise.
  #                            Use for sanity checks or when reward comes from external source.
  #   gsm8k_reward_func      - Binary (1/0). Extracts numeric answer, compares to ground truth.
  #                            Use for GSM8K and similar grade-school math datasets.
  #   math_reward_func       - Binary (1/0). Extracts \boxed{} answer, exact string match.
  #                            Use for MATH dataset / competition math.
  #   soft_math_reward_func  - Graduated (1.0/0.5/0.2). Partial credit for close answers.
  #                            Use when binary math reward is too sparse for learning.
  #   code_reward_func       - Binary (1/0). Runs generated code against test cases.
  #                            Use for MBPP / code-generation tasks. Requires test_cases in metadata.
  #   format_reward_func     - 0-1.0 in 0.25 increments (length, structure, no "I", EOS).
  #                            Use for instruction-following / style alignment tasks.
  #   mcqa_reward_func       - Binary (1/0). Extracts answer letter, compares to ground truth.
  #                            Use for MMLU-Pro / multiple-choice QA.
  #   reasoning_reward_func  - 0-1.0 graduated. Rewards <think>/<answer> tags + correctness.
  #                            Use for DeepSeek-R1 style chain-of-thought training.
  #   multimodal_reward_func - 0-1.0. Correctness + 0.2 modality awareness fallback.
  #                            Use for vision/audio tasks where modality grounding matters.
  #   rm_reward_func         - Continuous. Scores via trained reward model (value head).
  #                            Use for RLHF with a trained reward model. Requires reward_model_path.
  reward_func: default_reward_func

rollout:
  temperature: 1.0
  max_tokens: 512
  n_samples: 8
  top_p: 1.0
  top_k: -1
  ignore_eos: False
  stop: ""
  gpu_memory_utilization: 0.5
  stop_token_ids: []
  prompt_logprobs: False
  # Enforces unbiased sampling (temp=1, top_p=1, top_k=-1). Controls sampling
  # purity, not how many gradient steps per rollout (see train_steps_per_epoch).
  force_strict_on_policy: True
  # if model fits on one GPU, set it to 1, otherwise increase it until fits.
  tensor_parallel_size: 1
  # Batch size for the prompt dataloader during rollout generation.
  rollout_batch_size_per_gpu: 2

model:
  name: "google/gemma-3-1b-it"
  dtype: "bfloat16"
  ref_model: ""
  ref_model_offload_to_cpu: True
  trust_remote_code: False
  use_cache: False
  model_class: "llm"
  gradient_checkpointing: True
  # flash_attention_2 or eager
  attn_implementation: "flash_attention_2"

data:
  train_dnames: ["gsm8k_123245_ns_train"]
  train_ratios: {"gsm8k_123245_ns_train": 1.0}
  train_files_path: "./data/train.parquet"
  val_files_path: "./data/val.parquet"
  num_workers: 4
  max_seq_len: 512
  prompt_key: "prompt"
  answer_key: "answer"
 
deepspeed:
  zero_optimization:
    # 0:Disabled, 1:OptState, 2:Opt+Grad, 3:Opt+Grad+Param (Max VRAM savings)
    stage: 3

    # Threshold for keeping small parameters on GPU. Default 1e8.
    # Prevents "thrashing" (constant PCIe transfer) for small layers (like LayerNorm).
    stage3_param_persistence_threshold: 100000.0

    # Controls how much data is pre-fetched from CPU to GPU.
    # Larger = faster but more VRAM.
    stage3_prefetch_bucket_size: 50000000.0

    # --- CPU Offloading ---
    # Offload optimizer to CPU. Use "none" if GPU VRAM suffices.
    offload_optimizer:
      device: "none"
      pin_memory: true

    # Offload params to CPU. High performance penalty; use "none" if possible.
    offload_param:
      device: "none"
      pin_memory: true

    # --- Communication ---
    # Reduces fragmentation by copying gradients to contiguous buffer
    contiguous_gradients: true

    # Attempts to overlap the reduction (communication) of gradients
    # with the backward pass computation. Boosts speed.
    overlap_comm: true

    # In ZeRO, "reduce scatter" is how gradients are averaged and scattered
    # to the specific GPU responsible for updating that specific weight.
    reduce_scatter: true

    # --- Tuning Knobs ---
    # Controls the size of the chunks of data sent between GPUs.
    # Larger = higher bandwidth efficiency but higher peak memory usage.
    # 5e8 (500MB) is a standard aggressive default.
    reduce_bucket_size: 500000000.0
    allgather_bucket_size: 500000000.0

    # --- Saving / Checkpointing ---
    # If true, gathers the fragmented ZeRO-3 weights into a single FP16 file when saving
    stage3_gather_16bit_weights_on_model_save: true

  # --- Activation Checkpointing ---
  # Trade compute for VRAM by recomputing activations for the backward pass
  activation_checkpointing:
    # Only stores the activation for the specific partition of the model handled by this GPU
    partition_activations: true

    # Ensures activation memory is contiguous, reducing fragmentation.
    contiguous_memory_optimization: true

  # --- Logging ---
  steps_per_print: 100
  # If true, prints a detailed timing breakdown of fwd/bwd/step time.
  # Useful for debugging speed, but noisy.
  wall_clock_breakdown: false

  # Measure Model Flops Utilization
  flops_profiler:
    enabled: false
    profile_step: 10
    module_depth: -1
    top_modules: 1
    detailed: true
    output_file: null

inference_engine:
  name: "vllm"